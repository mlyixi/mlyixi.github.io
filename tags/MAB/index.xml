<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MAB on mlyixi的博客</title><link>https://mlyixi.github.io/tags/MAB/</link><description>Recent content in MAB on mlyixi的博客</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Sat, 11 Oct 2014 14:08:52 +0800</lastBuildDate><atom:link href="https://mlyixi.github.io/tags/MAB/index.xml" rel="self" type="application/rss+xml"/><item><title>多臂赌博机3</title><link>https://mlyixi.github.io/post/ml/%E5%A4%9A%E8%87%82%E8%B5%8C%E5%8D%9A%E6%9C%BA3/</link><pubDate>Sat, 11 Oct 2014 14:08:52 +0800</pubDate><guid>https://mlyixi.github.io/post/ml/%E5%A4%9A%E8%87%82%E8%B5%8C%E5%8D%9A%E6%9C%BA3/</guid><description>在上两节我们讨论的UCB系列算法面对的情况是静态的,即各臂的分布参数不会改变,于是我们就&amp;quot;乐观地面对不确定性&amp;quot;&amp;ndash;根据采样平均值尽快地确定那个最好的臂. 但是在现实世界中收益结构是更复杂的,非静态的.特别是当它涉及到竞争的场景,如股票交易.我们称之为对</description></item><item><title>多臂赌博机2</title><link>https://mlyixi.github.io/post/ml/%E5%A4%9A%E8%87%82%E8%B5%8C%E5%8D%9A%E6%9C%BA2/</link><pubDate>Fri, 10 Oct 2014 14:08:52 +0800</pubDate><guid>https://mlyixi.github.io/post/ml/%E5%A4%9A%E8%87%82%E8%B5%8C%E5%8D%9A%E6%9C%BA2/</guid><description>这一节我们来了解下多臂赌博机问题的提出和理论基础,最后讨论下UCB系列策略.当然,这里的多臂赌博机问题是随机式的. 随机式多臂赌博机的问题描述就不在这里重复了,可以参考上一节 理论 问题的证明 Lai &amp;amp; Robbins在1985年论证了对于某些特定的分布(只有一个实参的分布),存在有策略使得它</description></item><item><title>多臂赌博机1</title><link>https://mlyixi.github.io/post/ml/%E5%A4%9A%E8%87%82%E8%B5%8C%E5%8D%9A%E6%9C%BA1/</link><pubDate>Sat, 04 Oct 2014 14:08:52 +0800</pubDate><guid>https://mlyixi.github.io/post/ml/%E5%A4%9A%E8%87%82%E8%B5%8C%E5%8D%9A%E6%9C%BA1/</guid><description>假想一个风投他想着他的收益最大化,这时他总会面临一个两难: 何时去投资那些已经成功的公司,何时去投资那些还没有成功但具有很大潜力的公司.这里套用股市里的一句话:收益总是伴随着风险的. 一个成功的风投必须处理好这个勘探-开发两难(exploration and exploitation tradeoff): 勘探过多意味着不能获得较高的</description></item><item><title>pymaBandit说明</title><link>https://mlyixi.github.io/post/ml/pymaBandit%E8%AF%B4%E6%98%8E/</link><pubDate>Tue, 02 Sep 2014 14:08:52 +0800</pubDate><guid>https://mlyixi.github.io/post/ml/pymaBandit%E8%AF%B4%E6%98%8E/</guid><description>分布 Bernoulli distribution Poisson distribution Exponential distribution 策略 Gittin&amp;rsquo;s Bayesian optimal strategy for binary rewards [1] The classical UCB policy [2] The UCB-V policy [3] The KL-UCB policy [4] The Clopper-Pearson policy for binary rewards [4] The MOSS policy [5] The DMED policy [6] The Emipirical Likelihood UCB [7] The Bayes-UCB policy [8] The Thompson sampling policy [9] [1] Bandit Processes and Dynamic Allocation Indices J. C. Gittins. Journal of the Royal Statistical Society. Series B (Methodological) Vol. 41, No. 2. 1979 pp. 148–177 [2] Finite-time analysis of the multiarmed bandit problem Peter Auer, Nicolò Cesa-Bianchi and Paul Fischer. Machine Learning 47 2002 pp.235-256 [3] Exploration-exploitation trade-off using variance estimates in multi-armed bandits J.-Y. Audibert, R. Munos, Cs. Szepesvár</description></item></channel></rss>